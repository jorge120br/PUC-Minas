{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams.update({'figure.figsize':(16,3), 'figure.dpi':100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do arquivo Excel\n",
    "series = pd.read_excel('spare-parts-sales.xlsx', header=0, \n",
    "                       names=['item', 'sales', 'cost', 'date'], index_col=None, parse_dates=True, squeeze=True)\n",
    "\n",
    "# Filtra o arquivo para os anos de 2014 a 2016\n",
    "series['date'] = pd.to_datetime(series['date'])\n",
    "date_filter = (series['date'] >= '2014-01-01') & (series['date'] <= '2016-11-30')\n",
    "series = series[date_filter]\n",
    "\n",
    "# Filtra para o item '98550154' (óleo de motor)\n",
    "sku = '98550154'\n",
    "series = series[series.item == sku]\n",
    "\n",
    "# Exclui a coluna item e cost\n",
    "series.drop(series.columns[[0, 2]], axis=1, inplace=True)\n",
    "\n",
    "# Transforma para vendas semanais\n",
    "series = (series.groupby(pd.Grouper(key='date',freq='W')).sum()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformar lista em formato de aprendizagem supervisionada\n",
    "def series_to_supervised(data, n_in, n_out=1):\n",
    "    df = DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    # drop rows with NaN values\n",
    "    agg.dropna(inplace=True)\n",
    "    return agg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.DataFrame(series_to_supervised(series['sales'], 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em conjuntos de treinamento e teste\n",
    "X = df.values\n",
    "\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "train_size = int(len(X) * 0.66)\n",
    "train, test = X[0:train_size], X[train_size:]\n",
    "train_X, train_y = train[:,0], train[:,1]\n",
    "test_X, test_y = test[:,0], test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-participant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construção da grade de parâmetros com base nos resultados da pesquisa aleatória \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [0, 1],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "# Instanciando o modelo\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Iniciando o modelo de Grid Search\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-indian",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Ajustando os dados ao modelo\n",
    "grid_search.fit(train_X, train_y.ravel())\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_y)\n",
    "    MAPE = mean_absolute_percentage_error(test_y, predictions)\n",
    "    MAE = mean_absolute_error(test_y, predictions)\n",
    "    MSE = mean_squared_error(test_y, predictions)\n",
    "    R2 = r2_score(test_y,predictions)\n",
    "    accuracy = 100 - MAPE\n",
    "    print('Performance do Modelo')\n",
    "    print('----------------------------')\n",
    "    #print('Coeficiente de Determinação: {:0.2}.'.format(R2))\n",
    "    print('Acurácia = {:0.2f} %.'.format(accuracy))\n",
    "    print('MAPE = {:0.2f} %.'.format(MAPE))\n",
    "    print('MAE = {:0.2f} Unidades.'.format(MAE))\n",
    "    print('MSE = {:0.2f}.'.format(MSE))\n",
    "    print('RMSE = {:0.2f}.'.format(sqrt(MSE)))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-tuner",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n",
    "grid_accuracy = evaluate(best_grid, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_grid.predict(test_y)\n",
    "residuals = [test_y[i]-predictions[i] for i in range(len(predictions))]\n",
    "residuals = DataFrame(residuals)\n",
    "print('Descrição dos Resíduos')\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "original=np.concatenate((train_X, test_X), axis=0)\n",
    "plt.plot(original, label='Original')\n",
    "x = range((len(train_X)), (len(original)))\n",
    "plt.plot(x, predictions, label='Predicted')\n",
    "plt.title('Vendas de Peças de Reposição de 2014 a 2016')\n",
    "plt.xlabel('Meses')\n",
    "plt.ylabel('Quantidade Vendas')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-going",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
